{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05.MRC_실습.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터 다운로드 및 저장"
      ],
      "metadata": {
        "id": "jDfA_OfpkQVy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXxHL3rn1Qfu"
      },
      "outputs": [],
      "source": [
        "!mkdir ./data\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json -P ./data\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json -P ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 트랜스포머 라이브러리 설치"
      ],
      "metadata": {
        "id": "quH-YM90kMb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Elahy1OF196M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지, 라이브러리, 유틸리티 함수"
      ],
      "metadata": {
        "id": "-4PaMWl-kBXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import logging\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import timeit\n",
        "\n",
        "from collections import defaultdict, Counter, namedtuple, OrderedDict\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import List, Optional, Union, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    BertConfig,\n",
        "    BertModel,\n",
        "    BertPreTrainedModel,\n",
        "    BertTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    WEIGHTS_NAME\n",
        ")\n",
        "\n",
        "from transformers.models.bert import BasicTokenizer\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def to_list(tensor: torch.tensor) -> List:\n",
        "    return tensor.detach().cpu().tolist()"
      ],
      "metadata": {
        "id": "dRL_G5KpkAlc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 타입 및 구조 확인"
      ],
      "metadata": {
        "id": "H2Vm_hPpVXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join('./data', 'KorQuAD_v1.0_train.json'), 'r', encoding='utf-8') as fin:\n",
        "    train_data = json.load(fin)\n",
        "\n",
        "\"\"\"\n",
        "[실습]\n",
        "데이터의 타입 및 구조를 확인하시오.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "frWySZ_D2_Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리: raw data -> examples -> features"
      ],
      "metadata": {
        "id": "WEK139gIoTr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw 데이터 정형화 (raw data -> examples)\n",
        "## class KorQuADExample\n",
        "> 정형화된 데이터를 저장하는 클래스"
      ],
      "metadata": {
        "id": "JzPnbcZVVS_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KorQuADExample(object):\n",
        "    def __init__(self, qas_id, question_text, context_text, answer_text, start_position_character, title, answers=[], is_impossible=False, ):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_text = question_text\n",
        "        self.context_text = context_text\n",
        "        self.answer_text = answer_text\n",
        "        self.title = title\n",
        "        self.is_impossible = is_impossible\n",
        "        self.answers = answers\n",
        "\n",
        "        self.start_position, self.end_position = 0, 0\n",
        "\n",
        "        doc_tokens = []\n",
        "        char_to_word_offset = []\n",
        "        prev_is_whitespace = True\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        doc_tokens와 char_to_word_offset 리스트를 구축하시오.\n",
        "\n",
        "        doc_tokens: 어절 리스트\n",
        "        char_to_word_offset: 각 어절의 position\n",
        "\n",
        "        ex. 오늘은 마지막 교육 날이다.\n",
        "        doc_tokens = ['오늘은', '마지막', '교육', '날이다.']\n",
        "        char_to_word_offset = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        self.doc_tokens = doc_tokens\n",
        "        self.char_to_word_offset = char_to_word_offset\n",
        "\n",
        "        # Train Case\n",
        "        if start_position_character is not None and not is_impossible:\n",
        "            self.start_position = char_to_word_offset[start_position_character]\n",
        "            self.end_position = char_to_word_offset[min(start_position_character + len(answer_text) - 1, len(char_to_word_offset) - 1)]\n",
        "    \n",
        "    def _is_whitespace(self, c):\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "gyUb6pifU9-R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class KorQuADPreprocessor\n",
        "> 데이터를 읽고, 정형화하는 클래스"
      ],
      "metadata": {
        "id": "SbE3TUeUWq7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class KorQuADProcessor:\n",
        "\n",
        "    def get_train_examples(self, data_dir, filename):\n",
        "        with open(os.path.join(data_dir, filename), \"r\", encoding=\"utf-8\") as reader:\n",
        "            input_data = json.load(reader)[\"data\"][:20]\n",
        "\n",
        "        return self._create_examples(input_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir, filename):\n",
        "        with open(os.path.join(data_dir, filename), \"r\", encoding=\"utf-8\") as reader:\n",
        "            input_data = json.load(reader)[\"data\"][:2]\n",
        "\n",
        "        return self._create_examples(input_data, \"dev\")\n",
        "\n",
        "    def _create_examples(self, input_data, set_type):\n",
        "        is_training = set_type == \"train\"\n",
        "        examples = []\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        input_data를 KorQuADExample 형태로 정형화하시오.\n",
        "\n",
        "        c.f. is_impossible = input_data['data'][i]]['paragraphs'][j]['qas'][k][is_impossible']\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        return examples"
      ],
      "metadata": {
        "id": "IIpNZUORVkwa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습 결과 출력"
      ],
      "metadata": {
        "id": "lhhfU1nCSp3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join('./data', 'KorQuAD_v1.0_dev.json'), 'r', encoding='utf-8') as fin:\n",
        "    dev_data = json.load(fin)\n",
        "sample_data = [dev_data['data'][0]]\n",
        "\n",
        "exp_preprocessor = KorQuADProcessor()\n",
        "sample_example = exp_preprocessor._create_examples(sample_data, 'train')\n",
        "\n",
        "print(len(sample_example))\n",
        "print(type(sample_example))\n",
        "print()\n",
        "\n",
        "print(sample_example[0].qas_id)\n",
        "print(sample_example[0].question_text)\n",
        "print(sample_example[0].context_text)\n",
        "print(sample_example[0].answer_text)\n",
        "print(sample_example[0].title)\n",
        "print(sample_example[0].answers)\n",
        "print(sample_example[0].start_position)\n",
        "print(sample_example[0].end_position)\n",
        "print(sample_example[0].doc_tokens)\n",
        "print(sample_example[0].char_to_word_offset)"
      ],
      "metadata": {
        "id": "sixr99etpJj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리 및 텐서 데이터 구축 (examples -> features)"
      ],
      "metadata": {
        "id": "mSq3GlUDme-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n",
        "    \"\"\"\n",
        "    학습을 위한 start, end position을 wordpiece 단위로 조정하는 함수\n",
        "    \"\"\"\n",
        "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
        "\n",
        "    for new_start in range(input_start, input_end + 1):\n",
        "        for new_end in range(input_end, new_start - 1, -1):\n",
        "            text_span = \" \".join(doc_tokens[new_start : (new_end + 1)])\n",
        "            if text_span == tok_answer_text:\n",
        "                return (new_start, new_end)\n",
        "\n",
        "    return (input_start, input_end)\n",
        "\n",
        "def _new_check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n",
        "        if position < doc_span[\"start\"]:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span[\"start\"]\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index"
      ],
      "metadata": {
        "id": "X-eYFdQRmnFW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class KorQuADFeatures"
      ],
      "metadata": {
        "id": "7Vo6QhbQnbH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KorQuADFeatures:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        cls_index,\n",
        "        p_mask,\n",
        "        example_index,\n",
        "        unique_id,\n",
        "        paragraph_len,\n",
        "        token_is_max_context,\n",
        "        tokens,\n",
        "        token_to_orig_map,\n",
        "        start_position,\n",
        "        end_position,\n",
        "        is_impossible,\n",
        "        qas_id = None,\n",
        "    ):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.cls_index = cls_index\n",
        "        self.p_mask = p_mask\n",
        "\n",
        "        self.example_index = example_index\n",
        "        self.unique_id = unique_id\n",
        "        self.paragraph_len = paragraph_len\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.tokens = tokens\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "        self.is_impossible = is_impossible\n",
        "        self.qas_id = qas_id"
      ],
      "metadata": {
        "id": "WimorU4Fnee3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def KorQuAD_convert_example_to_features"
      ],
      "metadata": {
        "id": "WoMGhzR1nNDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KorQuAD_convert_example_to_features(example, max_seq_length, doc_stride, max_query_length, is_training):\n",
        "    features = []\n",
        "    if is_training and not example.is_impossible:\n",
        "        # Get start and end position\n",
        "        start_position = example.start_position\n",
        "        end_position = example.end_position\n",
        "\n",
        "        # If the answer cannot be found in the text, then skip this example.\n",
        "        actual_text = \" \".join(example.doc_tokens[start_position : (end_position + 1)])\n",
        "        cleaned_answer_text = \" \".join(whitespace_tokenize(example.answer_text))\n",
        "        if actual_text.find(cleaned_answer_text) == -1:\n",
        "            logger.warning(f\"Could not find answer: '{actual_text}' vs. '{cleaned_answer_text}'\")\n",
        "            return []\n",
        "\n",
        "    tok_to_orig_index = []\n",
        "    orig_to_tok_index = []\n",
        "    all_doc_tokens = []\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    all_doc_tokens, tok_to_orig_index, orig_to_tok_index 리스트를 구축하고,\n",
        "    실제 정답과 어절 단위 정답이 불일치할 경우, _improve_answer_span 함수를 이용해\n",
        "    wordpiece 단위의 start, end position을 구하시오.\n",
        "\n",
        "    c.f.\n",
        "    all_doc_tokens: Bert Tokenizer를 이용해 wordpiece 단위의 토큰 리스트\n",
        "    tok_to_orig_index: wordpiece 단위 토큰의 어절 단위 인덱스 값\n",
        "    orig_to_tok_index: 각 어절의 wordpiece 시작 인덱스 값\n",
        "\n",
        "    ex. \"1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의\"\n",
        "    all_doc_tokens = ['1989년', '2월', '15일', '여', '##의', '##도', '농', '##민', '폭', '##력', '시', '##위를', '주', '##도', '##한', '혐', '##의']\n",
        "    tok_to_orig_index = [0, 1, 2, 3, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 7, 8, 8]\n",
        "    orig_to_tok_index = [0, 1, 2, 3, 6, 8, 10, 12, 15]\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    \"\"\"\n",
        "    c.f.\n",
        "    BERT 입력 길이만큼 입력 시퀀스를 자르고,\n",
        "    입력 시퀀스 중 context 내에 정답이 있는 경우 그대로 사용,\n",
        "    입력 시퀀스 내에 정답이 없는 경우 doc_stride 만큼 입력 시퀀스의 context 조정하는 코드\n",
        "    \"\"\"\n",
        "    spans = []\n",
        "    truncated_query = tokenizer.encode(example.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)\n",
        "\n",
        "    sequence_added_tokens = 2 # [CLS] context [SEP]\n",
        "    sequence_pair_added_tokens = 3 # [CLS] context1 [SEP] context2 [SEP]\n",
        "\n",
        "    span_doc_tokens = all_doc_tokens\n",
        "    while len(spans) * doc_stride < len(all_doc_tokens):\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            truncated_query,\n",
        "            span_doc_tokens,\n",
        "            truncation=\"only_second\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_seq_length,\n",
        "            return_overflowing_tokens=True,\n",
        "            stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "        paragraph_len = min(\n",
        "            len(all_doc_tokens) - len(spans) * doc_stride,\n",
        "            max_seq_length - len(truncated_query) - sequence_pair_added_tokens,\n",
        "        )\n",
        "\n",
        "        if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n",
        "            non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n",
        "        else:\n",
        "            non_padded_ids = encoded_dict[\"input_ids\"]\n",
        "\n",
        "        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n",
        "\n",
        "        token_to_orig_map = {}\n",
        "        for i in range(paragraph_len):\n",
        "            index = len(truncated_query) + sequence_added_tokens + i\n",
        "            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]\n",
        "\n",
        "        encoded_dict[\"paragraph_len\"] = paragraph_len\n",
        "        encoded_dict[\"tokens\"] = tokens\n",
        "        encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n",
        "        encoded_dict[\"truncated_query_with_special_tokens_length\"] = len(truncated_query) + sequence_added_tokens\n",
        "        encoded_dict[\"token_is_max_context\"] = {}\n",
        "        encoded_dict[\"start\"] = len(spans) * doc_stride\n",
        "        encoded_dict[\"length\"] = paragraph_len\n",
        "\n",
        "        spans.append(encoded_dict)\n",
        "\n",
        "        if \"overflowing_tokens\" not in encoded_dict or (\n",
        "            \"overflowing_tokens\" in encoded_dict and len(encoded_dict[\"overflowing_tokens\"]) == 0\n",
        "        ):\n",
        "            break\n",
        "        span_doc_tokens = encoded_dict[\"overflowing_tokens\"]\n",
        "\n",
        "\n",
        "    for doc_span_index in range(len(spans)):\n",
        "        for j in range(spans[doc_span_index][\"paragraph_len\"]):\n",
        "            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)\n",
        "            index = spans[doc_span_index][\"truncated_query_with_special_tokens_length\"] + j\n",
        "            spans[doc_span_index][\"token_is_max_context\"][index] = is_max_context\n",
        "\n",
        "    for span in spans:\n",
        "        cls_index = span[\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "\n",
        "        p_mask = np.ones_like(span[\"token_type_ids\"])\n",
        "        p_mask[len(truncated_query) + sequence_added_tokens :] = 0\n",
        "\n",
        "        pad_token_indices = np.where(span[\"input_ids\"] == tokenizer.pad_token_id)\n",
        "        special_token_indices = np.asarray(\n",
        "            tokenizer.get_special_tokens_mask(span[\"input_ids\"], already_has_special_tokens=True)\n",
        "        ).nonzero()\n",
        "\n",
        "        p_mask[pad_token_indices] = 1\n",
        "        p_mask[special_token_indices] = 1\n",
        "        p_mask[cls_index] = 0\n",
        "\n",
        "        span_is_impossible = example.is_impossible\n",
        "        start_position = 0\n",
        "        end_position = 0\n",
        "        if is_training and not span_is_impossible:\n",
        "            doc_start = span[\"start\"]\n",
        "            doc_end = span[\"start\"] + span[\"length\"] - 1\n",
        "            out_of_span = False\n",
        "\n",
        "            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):\n",
        "                out_of_span = True\n",
        "\n",
        "            if out_of_span:\n",
        "                start_position = cls_index\n",
        "                end_position = cls_index\n",
        "                span_is_impossible = True\n",
        "            else:\n",
        "                doc_offset = len(truncated_query) + sequence_added_tokens\n",
        "\n",
        "                start_position = tok_start_position - doc_start + doc_offset\n",
        "                end_position = tok_end_position - doc_start + doc_offset\n",
        "\n",
        "        features.append(\n",
        "            KorQuADFeatures(\n",
        "                span[\"input_ids\"],\n",
        "                span[\"attention_mask\"],\n",
        "                span[\"token_type_ids\"],\n",
        "                cls_index,\n",
        "                p_mask.tolist(),\n",
        "                example_index=0, \n",
        "                unique_id=0,\n",
        "                paragraph_len=span[\"paragraph_len\"],\n",
        "                token_is_max_context=span[\"token_is_max_context\"],\n",
        "                tokens=span[\"tokens\"],\n",
        "                token_to_orig_map=span[\"token_to_orig_map\"],\n",
        "                start_position=start_position,\n",
        "                end_position=end_position,\n",
        "                is_impossible=span_is_impossible,\n",
        "                qas_id=example.qas_id,\n",
        "            )\n",
        "        )\n",
        "    return features"
      ],
      "metadata": {
        "id": "WKqIZ0J5nMwC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def KorQuAD_convert_examples_to_features"
      ],
      "metadata": {
        "id": "F7vbDNOEnxw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KorQuAD_convert_example_to_features_init(tokenizer_for_convert):\n",
        "    global tokenizer\n",
        "    tokenizer = tokenizer_for_convert\n",
        "\n",
        "def KorQuAD_convert_examples_to_features(\n",
        "    examples,\n",
        "    tokenizer,\n",
        "    max_seq_length,\n",
        "    doc_stride,\n",
        "    max_query_length,\n",
        "    is_training,\n",
        "):\n",
        "    features = []\n",
        "\n",
        "    # Multiprocessing을 통한 features list 구축\n",
        "    with Pool(os.cpu_count(), initializer=KorQuAD_convert_example_to_features_init, initargs=(tokenizer,)) as p:\n",
        "        annotate_ = partial(\n",
        "            KorQuAD_convert_example_to_features,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=doc_stride,\n",
        "            max_query_length=max_query_length,\n",
        "            is_training=is_training,\n",
        "        )\n",
        "        features = list(\n",
        "            tqdm(\n",
        "                p.imap(annotate_, examples, chunksize=32),\n",
        "                total=len(examples),\n",
        "                desc=\"convert KorQuAD examples to features\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    new_features = []\n",
        "    unique_id = 1000000000\n",
        "    example_index = 0\n",
        "    for example_features in tqdm(features, total=len(features), desc=\"add example index and unique id\"):\n",
        "        if not example_features:\n",
        "            continue\n",
        "        for example_feature in example_features:\n",
        "            example_feature.example_index = example_index\n",
        "            example_feature.unique_id = unique_id\n",
        "            new_features.append(example_feature)\n",
        "            unique_id += 1\n",
        "        example_index += 1\n",
        "    features = new_features\n",
        "    del new_features\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    torch.tensor 자료형의\n",
        "\n",
        "    all_input_ids,\n",
        "    all_attention_masks,\n",
        "    all_token_type_ids,\n",
        "    all_cls_index,\n",
        "    all_p_mask,\n",
        "    all_is_impossible\n",
        "\n",
        "    을 구축하시오.\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    if not is_training:\n",
        "        all_feature_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_attention_masks, all_token_type_ids, all_feature_index, all_cls_index, all_p_mask)\n",
        "    else:\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        torch.tensor 자료형의\n",
        "\n",
        "        all_start_positions,\n",
        "        all_end_positions\n",
        "\n",
        "        를 구축하시오.\n",
        "        \"\"\"\n",
        "        \n",
        "        dataset = TensorDataset(\n",
        "            all_input_ids,\n",
        "            all_attention_masks,\n",
        "            all_token_type_ids,\n",
        "            all_start_positions,\n",
        "            all_end_positions,\n",
        "            all_cls_index,\n",
        "            all_p_mask,\n",
        "            all_is_impossible,\n",
        "        )\n",
        "\n",
        "    return features, dataset"
      ],
      "metadata": {
        "id": "IXxegOezmhIJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습 출력"
      ],
      "metadata": {
        "id": "2d4MFrDfxDmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "sample_max_seq_length = 384\n",
        "sample_max_query_length = 64\n",
        "sample_doc_stride = 128\n",
        "\n",
        "sample_features, sample_dataset = KorQuAD_convert_examples_to_features(\n",
        "    examples=[sample_example[1]],\n",
        "    tokenizer=sample_tokenizer,\n",
        "    max_seq_length=sample_max_seq_length,\n",
        "    doc_stride=sample_doc_stride,\n",
        "    max_query_length=sample_max_query_length,\n",
        "    is_training=True,\n",
        ")"
      ],
      "metadata": {
        "id": "rHsWnfX6x5h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 후처리"
      ],
      "metadata": {
        "id": "X63ZBeo-5b53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KorQuADResult:\n",
        "    def __init__(self, unique_id, start_logits, end_logits):\n",
        "        self.start_logits = start_logits\n",
        "        self.end_logits = end_logits\n",
        "        self.unique_id = unique_id"
      ],
      "metadata": {
        "id": "ODDfdnHVa63o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "\n",
        "    max_score = None\n",
        "    for score in scores:\n",
        "        if max_score is None or score > max_score:\n",
        "            max_score = score\n",
        "\n",
        "    exp_scores = []\n",
        "    total_sum = 0.0\n",
        "    for score in scores:\n",
        "        x = math.exp(score - max_score)\n",
        "        exp_scores.append(x)\n",
        "        total_sum += x\n",
        "\n",
        "    probs = []\n",
        "    for score in exp_scores:\n",
        "        probs.append(score / total_sum)\n",
        "    return probs\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case):\n",
        "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "    def _strip_spaces(text):\n",
        "        ns_chars = []\n",
        "        ns_to_s_map = OrderedDict()\n",
        "        for i, c in enumerate(text):\n",
        "            if c == \" \":\n",
        "                continue\n",
        "            ns_to_s_map[len(ns_chars)] = i\n",
        "            ns_chars.append(c)\n",
        "        ns_text = \"\".join(ns_chars)\n",
        "        return (ns_text, ns_to_s_map)\n",
        "\n",
        "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "    start_position = tok_text.find(pred_text)\n",
        "    if start_position == -1:\n",
        "        return orig_text\n",
        "        \n",
        "    end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "    if len(orig_ns_text) != len(tok_ns_text):\n",
        "        return orig_text\n",
        "\n",
        "    tok_s_to_ns_map = {}\n",
        "    for i, tok_index in tok_ns_to_s_map.items():\n",
        "        tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "    orig_start_position = None\n",
        "    if start_position in tok_s_to_ns_map:\n",
        "        ns_start_position = tok_s_to_ns_map[start_position]\n",
        "        if ns_start_position in orig_ns_to_s_map:\n",
        "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "    if orig_start_position is None:\n",
        "        return orig_text\n",
        "\n",
        "    orig_end_position = None\n",
        "    if end_position in tok_s_to_ns_map:\n",
        "        ns_end_position = tok_s_to_ns_map[end_position]\n",
        "        if ns_end_position in orig_ns_to_s_map:\n",
        "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "    if orig_end_position is None:\n",
        "        return orig_text\n",
        "\n",
        "    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n",
        "    return output_text\n",
        "\n",
        "\n",
        "def compute_predictions_logits(\n",
        "    all_examples,\n",
        "    all_features,\n",
        "    all_results,\n",
        "    n_best_size,\n",
        "    max_answer_length,\n",
        "    do_lower_case,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    tokenizer,\n",
        "):\n",
        "    if output_prediction_file:\n",
        "        logger.info(f\"Writing predictions to: {output_prediction_file}\")\n",
        "    if output_nbest_file:\n",
        "        logger.info(f\"Writing nbest to: {output_nbest_file}\")\n",
        "\n",
        "    example_index_to_features = defaultdict(list)\n",
        "    for feature in all_features:\n",
        "        example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "    _PrelimPrediction = namedtuple( \n",
        "        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
        "    )\n",
        "\n",
        "    all_predictions = OrderedDict()\n",
        "    all_nbest_json = OrderedDict()\n",
        "    scores_diff_json = OrderedDict()\n",
        "\n",
        "    for example_index, example in enumerate(all_examples):\n",
        "        features = example_index_to_features[example_index]\n",
        "\n",
        "        prelim_predictions = []\n",
        "        # keep track of the minimum score of null start+end of position 0\n",
        "        score_null = 1000000  # large and positive\n",
        "        min_null_feature_index = 0  # the paragraph slice with min null score\n",
        "        null_start_logit = 0  # the start logit at the slice with min null score\n",
        "        null_end_logit = 0  # the end logit at the slice with min null score\n",
        "        for feature_index, feature in enumerate(features):\n",
        "            result = unique_id_to_result[feature.unique_id]\n",
        "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # We could hypothetically create invalid predictions, e.g., predict\n",
        "                    # that the start of the span is in the question. We throw out all\n",
        "                    # invalid predictions.\n",
        "                    if start_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if end_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if start_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if end_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if not feature.token_is_max_context.get(start_index, False):\n",
        "                        continue\n",
        "                    if end_index < start_index:\n",
        "                        continue\n",
        "                    length = end_index - start_index + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "                    prelim_predictions.append(\n",
        "                        _PrelimPrediction(\n",
        "                            feature_index=feature_index,\n",
        "                            start_index=start_index,\n",
        "                            end_index=end_index,\n",
        "                            start_logit=result.start_logits[start_index],\n",
        "                            end_logit=result.end_logits[end_index],\n",
        "                        )\n",
        "                    )\n",
        "        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
        "\n",
        "        _NbestPrediction = namedtuple( \n",
        "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
        "        )\n",
        "\n",
        "        seen_predictions = {}\n",
        "        nbest = []\n",
        "        for pred in prelim_predictions:\n",
        "            if len(nbest) >= n_best_size:\n",
        "                break\n",
        "            feature = features[pred.feature_index]\n",
        "            if pred.start_index > 0:  # this is a non-null prediction\n",
        "                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n",
        "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
        "\n",
        "                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
        "\n",
        "                # Clean whitespace\n",
        "                tok_text = tok_text.strip()\n",
        "                tok_text = \" \".join(tok_text.split())\n",
        "                orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "                final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
        "                if final_text in seen_predictions:\n",
        "                    continue\n",
        "\n",
        "                seen_predictions[final_text] = True\n",
        "            else:\n",
        "                final_text = \"\"\n",
        "                seen_predictions[final_text] = True\n",
        "\n",
        "            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n",
        "\n",
        "        if not nbest:\n",
        "            nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "        assert len(nbest) >= 1, \"No valid predictions\"\n",
        "\n",
        "        total_scores = []\n",
        "        best_non_null_entry = None\n",
        "        for entry in nbest:\n",
        "            total_scores.append(entry.start_logit + entry.end_logit)\n",
        "            if not best_non_null_entry:\n",
        "                if entry.text:\n",
        "                    best_non_null_entry = entry\n",
        "\n",
        "        probs = _compute_softmax(total_scores)\n",
        "\n",
        "        nbest_json = []\n",
        "        for i, entry in enumerate(nbest):\n",
        "            output = OrderedDict()\n",
        "            output[\"text\"] = entry.text\n",
        "            output[\"probability\"] = probs[i]\n",
        "            output[\"start_logit\"] = entry.start_logit\n",
        "            output[\"end_logit\"] = entry.end_logit\n",
        "            nbest_json.append(output)\n",
        "\n",
        "        assert len(nbest_json) >= 1, \"No valid predictions\"\n",
        "\n",
        "        score_diff = score_null - best_non_null_entry.start_logit - (best_non_null_entry.end_logit)\n",
        "        scores_diff_json[example.qas_id] = score_diff\n",
        "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "            \n",
        "        all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "    if output_prediction_file:\n",
        "        with open(output_prediction_file, \"w\", encoding='utf-8') as writer:\n",
        "            json.dump(all_predictions, writer, indent='\\t', ensure_ascii=False)\n",
        "\n",
        "    if output_nbest_file:\n",
        "        with open(output_nbest_file, \"w\", encoding='utf-8') as writer:\n",
        "            json.dump(all_nbest_json, writer, indent='\\t', ensure_ascii=False)\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "qr1qxX4f5eI4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_answer(s):    \n",
        "    def remove_(text):\n",
        "        text = re.sub(\"'\", \" \", text)\n",
        "        text = re.sub('\"', \" \", text)\n",
        "        text = re.sub('《', \" \", text)\n",
        "        text = re.sub('》', \" \", text)\n",
        "        text = re.sub('<', \" \", text)\n",
        "        text = re.sub('>', \" \", text) \n",
        "        text = re.sub('〈', \" \", text)\n",
        "        text = re.sub('〉', \" \", text)   \n",
        "        text = re.sub(\"\\(\", \" \", text)\n",
        "        text = re.sub(\"\\)\", \" \", text)\n",
        "        text = re.sub(\"‘\", \" \", text)\n",
        "        text = re.sub(\"’\", \" \", text)      \n",
        "        return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(remove_(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    f1-score를 구하시오.\n",
        "\n",
        "    c.f.\n",
        "    비교 단위: string\n",
        "    precision: (pred & true) / pred\n",
        "    recall: (pred & true) / true\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    EM score를 구하시오.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return em\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "def KorQuAD_evaluate(examples, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    for cnt, example in enumerate(examples):\n",
        "        total += 1\n",
        "        qas_id = example.qas_id\n",
        "        if qas_id not in predictions:\n",
        "            message = 'Unanswered question ' + qas_id + ' will receive score 0.'\n",
        "            print(message, file=sys.stderr)\n",
        "            continue\n",
        "        ground_truths = [answer[\"text\"] for answer in example.answers]\n",
        "        prediction = predictions[qas_id]\n",
        "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
        "        if cnt == 0 or qas_id == \"6332405-1-0\":\n",
        "            logger.info(\"ground truths: {}\".format(ground_truths))\n",
        "            logger.info(\"prediction: {}\".format(prediction))\n",
        "            logger.info(\"F1: {:.3f} || EM: {:.3f}\\n\".format(metric_max_over_ground_truths(f1_score, prediction, ground_truths), metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)))\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}"
      ],
      "metadata": {
        "id": "uuQxwPQ_51g-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로딩\n",
        "## def load_and_cache_examples\n",
        "> 정형화된 데이터가 저장되어 있는 경우 이를 불러오고,\n",
        "> 그렇지 않을 경우 데이터를 정형화한 후 저장(caching) 및 반환하는 함수"
      ],
      "metadata": {
        "id": "wAPbFy5Tja3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = args.data_dir if args.data_dir else \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        전처리를 위해 정의한 클래스와 함수를 이용해\n",
        "        example, features, dataset을 구축하시오.\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "liN_hOKhjY-d"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 정의: class BertForKorQuAD"
      ],
      "metadata": {
        "id": "YSHA3SbFZPlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForKorQuAD(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        start_position과 end_position을 학습하는 MLP를 선언하시오.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        start_positions: Optional[torch.Tensor] = None,\n",
        "        end_positions: Optional[torch.Tensor] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        bert로부터 입력 시퀀스의 모든 토큰에 대한 hidden_state를 받은 후,\n",
        "        MLP를 통해 각 토큰마다 start_logits와 end_logits를 계산하는 모델을 구현하시오.\n",
        "\n",
        "        c.f.\n",
        "        https://huggingface.co/docs/transformers/model_doc/bert\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            \"\"\"\n",
        "            [실습]\n",
        "            loss function을 선언한 후, total_loss를 구하시오.\n",
        "            \"\"\"\n",
        "            \n",
        "\n",
        "        output = (start_logits, end_logits) + outputs[2:]\n",
        "\n",
        "        return ((total_loss,) + output) if total_loss is not None else output"
      ],
      "metadata": {
        "id": "BIQLHNb_ZPOs"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate 함수 정의"
      ],
      "metadata": {
        "id": "Dn03KmSP2oxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            feature_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        for i, feature_index in enumerate(feature_indices):\n",
        "            eval_feature = features[feature_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "            output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "            start_logits, end_logits = output\n",
        "            result = KorQuADResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
        "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        args.n_best_size,\n",
        "        args.max_answer_length,\n",
        "        args.do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = KorQuAD_evaluate(examples, predictions)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "tkU_EwkX2whS"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train 함수 정의"
      ],
      "metadata": {
        "id": "V12B7GCa2lw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, train_dataset, model, tokenizer):\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. accumulation) = %d\",\n",
        "        args.train_batch_size * args.gradient_accumulation_steps\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss = 0.0\n",
        "\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    학습 프로세스를 구현하시오.\n",
        "    \"\"\"\n",
        "    \n",
        "        \n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "metadata": {
        "id": "AeEmLfqe2nu8"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main함수 정의"
      ],
      "metadata": {
        "id": "7uF2Qr4yXYxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n",
        "        logger.warning(\n",
        "            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n",
        "            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n",
        "            \"stride or increase the maximum length to ensure the features are correctly built.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
        "    logger.warning(\n",
        "        \"Process device: %s, n_gpu: %s\",\n",
        "        args.device,\n",
        "        args.n_gpu,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    # Load BERT configurations\n",
        "    config = BertConfig.from_pretrained(args.model_name_or_path)\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
        "    model = BertForKorQuAD.from_pretrained(args.model_name_or_path, config=config)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Train\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "        # Save the trained model and the tokenizer\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        model.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        model = BertForKorQuAD.from_pretrained(args.output_dir)\n",
        "        tokenizer = BertTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluate\n",
        "    results = {}\n",
        "    if args.do_eval:\n",
        "        logger.info(\"Loading checkpoints saved during training for evaluation\")\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.do_train and args.eval_all_checkpoints:\n",
        "                checkpoints = list(\n",
        "                    os.path.dirname(c)\n",
        "                    for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "                )\n",
        "\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            # Reload the model\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            model = BertForKorQuAD.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "\n",
        "            # Evaluate\n",
        "            result = evaluate(args, model, tokenizer, prefix=global_step)\n",
        "\n",
        "            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    logger.info(\"Results: {}\".format(results))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "eceThL3ZXaEq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arguments Parsing과 Main 함수 실행"
      ],
      "metadata": {
        "id": "gWBys9IwXFvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Arguments for several paths or file names\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default='bert-base-multilingual-cased',\n",
        "        type=str,\n",
        "        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        default=\"./data\",\n",
        "        type=str,\n",
        "        help=\"The input data dir. Should contain the .json files for the task.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=\"./outputs/\",\n",
        "        type=str,\n",
        "        help=\"The output directory where the model checkpoints and predictions will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_file\",\n",
        "        default=\"KorQuAD_v1.0_train.json\",\n",
        "        type=str,\n",
        "        help=\"The input training file. If a data dir is specified, will look for the file there\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--predict_file\",\n",
        "        default=\"KorQuAD_v1.0_dev.json\",\n",
        "        type=str,\n",
        "        help=\"The input evaluation file. If a data dir is specified, will look for the file there\",\n",
        "    )\n",
        "\n",
        "    # Hyperparameters\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "    parser.add_argument(\n",
        "        \"--max_seq_length\",\n",
        "        default=384,\n",
        "        type=int,\n",
        "        help=\"The maximum total input sequence length after WordPiece tokenization. Sequences \"\n",
        "        \"longer than this will be truncated, and sequences shorter than this will be padded.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--doc_stride\",\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_query_length\",\n",
        "        default=64,\n",
        "        type=int,\n",
        "        help=\"The maximum number of tokens for the question. Questions longer than this will \"\n",
        "        \"be truncated to this length.\",\n",
        "    )\n",
        "    parser.add_argument(\"--train_batch_size\", default=8, type=int, help=\"Batch size GPU/CPU for training.\")\n",
        "    parser.add_argument(\n",
        "        \"--eval_batch_size\", default=8, type=int, help=\"Batch size GPU/CPU for evaluation.\"\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--n_best_size\",\n",
        "        default=20,\n",
        "        type=int,\n",
        "        help=\"The total number of n-best predictions to generate in the nbest_predictions.json output file.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_answer_length\",\n",
        "        default=30,\n",
        "        type=int,\n",
        "        help=\"The maximum length of an answer that can be generated. This is needed because the start \"\n",
        "        \"and end predictions are not conditioned on one another.\",\n",
        "    )\n",
        "\n",
        "    # Actions\n",
        "    parser.add_argument(\"--do_train\", default=False, action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", default=True, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--do_lower_case\", default=False, action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Whether not to use CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\", default=True, action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\", default=False, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
        "    )\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Logging hyperparameters\n",
        "    logger.info(\"Training and evaluation parameters\")\n",
        "    for k, v in args.__dict__.items():\n",
        "        logger.info(\"{}: {}\".format(k, v))\n",
        "    \n",
        "    main(args)"
      ],
      "metadata": {
        "id": "U4y_DRefXE37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xs8pE_2frZR2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}